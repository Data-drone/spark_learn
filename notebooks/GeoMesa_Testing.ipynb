{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "measured-tolerance",
   "metadata": {},
   "source": [
    "# GeoMesa Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-browser",
   "metadata": {},
   "source": [
    "testing the geoprocessing capabilities of GeoMesa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "republican-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%init_spark\n",
    "launcher.master = \"spark://spark-master:7077\"\n",
    "launcher.num_executors = 2\n",
    "launcher.executor_cores = 8\n",
    "launcher.executor_memory = '16G'\n",
    "launcher.packages = [\"org.apache.hadoop:hadoop-aws:3.2.0\",\n",
    "                    \"org.locationtech.geomesa:geomesa-spark-jts_2.12:3.2.0\"]\n",
    "launcher.conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \n",
    "                  \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "formal-calgary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://7847ea2e55be:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = spark://spark-master:7077, app id = app-20210531023343-0000)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\n",
       "import org.locationtech.jts.geom._\n",
       "import org.locationtech.geomesa.spark.jts._\n",
       "import org.apache.spark.sql.SaveMode\n",
       "import org.apache.spark.sql.types._\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// load geo mesa bits\n",
    "import org.apache.spark.sql.types._\n",
    "import org.locationtech.jts.geom._\n",
    "import org.locationtech.geomesa.spark.jts._\n",
    "import org.apache.spark.sql.SaveMode\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intelligent-egyptian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6ab8cb77\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "allied-jordan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@6ab8cb77\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.withJTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-review",
   "metadata": {},
   "source": [
    "## Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "confident-startup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "green_trip_data_2015_h1: String = s3a://nyc-tlc/trip data/green_tripdata_2015-0[1-6].csv\n",
       "green_trip_2015_h1: org.apache.spark.sql.DataFrame = [VendorID: string, lpep_pickup_datetime: string ... 19 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val green_trip_data_2015_h1 = \"s3a://nyc-tlc/trip data/green_tripdata_2015-0[1-6].csv\"\n",
    "val green_trip_2015_h1 = spark.read.option(\"header\", true).csv(green_trip_data_2015_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "broad-vector",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- Lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- Store_and_fwd_flag: string (nullable = true)\n",
      " |-- RateCodeID: string (nullable = true)\n",
      " |-- Pickup_longitude: string (nullable = true)\n",
      " |-- Pickup_latitude: string (nullable = true)\n",
      " |-- Dropoff_longitude: string (nullable = true)\n",
      " |-- Dropoff_latitude: string (nullable = true)\n",
      " |-- Passenger_count: string (nullable = true)\n",
      " |-- Trip_distance: string (nullable = true)\n",
      " |-- Fare_amount: string (nullable = true)\n",
      " |-- Extra: string (nullable = true)\n",
      " |-- MTA_tax: string (nullable = true)\n",
      " |-- Tip_amount: string (nullable = true)\n",
      " |-- Tolls_amount: string (nullable = true)\n",
      " |-- Ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- Total_amount: string (nullable = true)\n",
      " |-- Payment_type: string (nullable = true)\n",
      " |-- Trip_type : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "green_trip_2015_h1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-score",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "powerful-colombia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned_green_trip_2015_h1: org.apache.spark.sql.DataFrame = [VendorID: string, lpep_pickup_datetime: string ... 21 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cleaned_green_trip_2015_h1 = green_trip_2015_h1\n",
    "    .withColumn(\"pickup_longitude\", col(\"Pickup_longitude\").cast(DoubleType))\n",
    "    .withColumn(\"pickup_latitude\", col(\"Pickup_latitude\").cast(DoubleType))\n",
    "    .withColumn(\"pickup_point\", st_makePoint(col(\"pickup_longitude\"), col(\"pickup_latitude\")))\n",
    "    .withColumn(\"dropoff_longitude\", col(\"Dropoff_longitude\").cast(DoubleType))\n",
    "    .withColumn(\"dropoff_latitude\", col(\"Dropoff_latitude\").cast(DoubleType))\n",
    "    .withColumn(\"dropoff_point\", st_makePoint(col(\"dropoff_longitude\"), col(\"dropoff_latitude\")))\n",
    "    .withColumn(\"trip_distance\", col(\"Trip_distance\").cast(DoubleType) )\n",
    "    .withColumn(\"passenger_count\", col(\"Passenger_count\").cast(IntegerType) )\n",
    "    .withColumn(\"total_amount\", col(\"Total_amount\").cast(DoubleType) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "developmental-aircraft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- Lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- Store_and_fwd_flag: string (nullable = true)\n",
      " |-- RateCodeID: string (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- Fare_amount: string (nullable = true)\n",
      " |-- Extra: string (nullable = true)\n",
      " |-- MTA_tax: string (nullable = true)\n",
      " |-- Tip_amount: string (nullable = true)\n",
      " |-- Tolls_amount: string (nullable = true)\n",
      " |-- Ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- Payment_type: string (nullable = true)\n",
      " |-- Trip_type : string (nullable = true)\n",
      " |-- pickup_point: point (nullable = true)\n",
      " |-- dropoff_point: point (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_green_trip_2015_h1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-exhaust",
   "metadata": {},
   "source": [
    "## Read in the Taxi Zones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-philippines",
   "metadata": {},
   "source": [
    "Note that this file needs to be made available to all the executors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changed-cisco",
   "metadata": {},
   "source": [
    "Get taxi zones from https://data.cityofnewyork.us/api/views/755u-8jsi/rows.csv?accessType=DOWNLOAD\n",
    "and make them accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bound-assembly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "taxi_zones: org.apache.spark.sql.DataFrame = [OBJECTID: int, Shape_Leng: double ... 5 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val taxi_zones = spark.read.option(\"header\", true)\n",
    "                    .option(\"inferSchema\", \"true\")\n",
    "                    .csv(\"/opt/spark-data/nyc_taxi_zones/taxi_zones.csv\")\n",
    "                    .withColumn(\"the_geom\", st_geomFromWKT(col(\"the_geom\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sapphire-clause",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OBJECTID: integer (nullable = true)\n",
      " |-- Shape_Leng: double (nullable = true)\n",
      " |-- the_geom: geometry (nullable = true)\n",
      " |-- Shape_Area: double (nullable = true)\n",
      " |-- zone: string (nullable = true)\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- borough: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "taxi_zones.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "julian-breast",
   "metadata": {},
   "outputs": [],
   "source": [
    "//taxi_zones.write.format(\"parquet\").mode(SaveMode.Overwrite).saveAsTable(\"wkt_taxi_zones\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "billion-junction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "merged: org.apache.spark.sql.DataFrame = [OBJECTID: int, Shape_Leng: double ... 28 more fields]\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val merged = taxi_zones.join(cleaned_green_trip_2015_h1, st_contains(col(\"the_geom\"), col(\"pickup_point\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "professional-working",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cleaned_merged: org.apache.spark.sql.DataFrame = [pickup_zone: string, pickup_borough: string ... 7 more fields]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cleaned_merged = merged\n",
    "            .select(\"zone\",\"borough\",\"pickup_point\", \"dropoff_point\",\"lpep_pickup_datetime\", \"lpep_dropoff_datetime\", \"total_amount\", \"passenger_count\", \"trip_distance\")\n",
    "            .withColumnRenamed(\"zone\", \"pickup_zone\")\n",
    "            .withColumnRenamed(\"borough\", \"pickup_borough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "active-product",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Array[org.apache.spark.sql.Row] = Array([2015-01-01 00:34:42,2015-01-01 00:38:34,Astoria,Queens,0.88,1,6.3])\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_merged.select(\"lpep_pickup_datetime\", \"lpep_dropoff_datetime\", \"pickup_zone\", \"pickup_borough\", \"trip_distance\", \"passenger_count\", \"total_amount\").take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "automatic-pepper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drop_off_merged: org.apache.spark.sql.DataFrame = [OBJECTID: int, Shape_Leng: double ... 14 more fields]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val drop_off_merged = taxi_zones.join(cleaned_merged, st_contains(col(\"the_geom\"), col(\"dropoff_point\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "nonprofit-ordinance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_merged: org.apache.spark.sql.DataFrame = [dropoff_zone: string, dropoff_borough: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val final_merged = drop_off_merged\n",
    "            .select(\"zone\",\"borough\",\"pickup_point\", \"pickup_zone\", \"pickup_borough\", \"dropoff_point\",\"lpep_pickup_datetime\", \n",
    "                    \"lpep_dropoff_datetime\", \"total_amount\", \"passenger_count\", \"trip_distance\")\n",
    "            .withColumnRenamed(\"zone\", \"dropoff_zone\")\n",
    "            .withColumnRenamed(\"borough\", \"dropoff_borough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "becoming-florist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res16: Array[org.apache.spark.sql.Row] = Array([2015-01-01 00:34:42,2015-01-01 00:38:34,Astoria,Queens,Astoria,Queens,0.88,1,6.3], [2015-01-01 00:34:46,2015-01-01 00:47:23,Crown Heights North,Brooklyn,Windsor Terrace,Brooklyn,3.08,1,13.3])\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_merged.select(\"lpep_pickup_datetime\", \"lpep_dropoff_datetime\", \"pickup_zone\", \"pickup_borough\", \"dropoff_zone\", \"dropoff_borough\", \"trip_distance\", \"passenger_count\", \"total_amount\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "compact-accessory",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted.",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted.",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)",
      "  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)",
      "  at org.apache.spark.sql.execution.datasources.DataSource.writeAndRead(DataSource.scala:548)",
      "  at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.saveDataIntoTable(createDataSourceTables.scala:220)",
      "  at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:177)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)",
      "  at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)",
      "  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)",
      "  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)",
      "  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)",
      "  at org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)",
      "  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)",
      "  at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:753)",
      "  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:731)",
      "  at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:626)",
      "  ... 50 elided",
      "Caused by: java.io.IOException: Failed to rename DeprecatedRawLocalFileStatus{path=file:/opt/spark-data/processed/taxi_processed/_temporary/0/task_202105270158384840079117273550646_0020_m_000010/part-00010-13fd8110-95e9-42c3-bc0e-bd88a9bb751e-c000.snappy.parquet; isDirectory=false; length=24847275; replication=1; blocksize=33554432; modification_time=1622090002000; access_time=1622080720138; owner=; group=; permission=rw-rw-rw-; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false} to file:/opt/spark-data/processed/taxi_processed/part-00010-13fd8110-95e9-42c3-bc0e-bd88a9bb751e-c000.snappy.parquet",
      "  at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:473)",
      "  at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.mergePaths(FileOutputCommitter.java:486)",
      "  at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:403)",
      "  at org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:375)",
      "  at org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:48)",
      "  at org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:182)",
      "  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:220)",
      "  ... 74 more",
      ""
     ]
    }
   ],
   "source": [
    "final_merged.write.format(\"parquet\")\n",
    "    .option(\"path\", \"/opt/spark-data/processed/taxi_processed\")\n",
    "    .mode(SaveMode.Overwrite).saveAsTable(\"taxi_processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-dream",
   "metadata": {},
   "source": [
    "# Stopping Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "// scala test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expected-cedar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cards: List[String] = List(a, b, c)\n",
       "frames: List[Int] = List(1, 2, 3)\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cards: List[String] = List(\"a\", \"b\", \"c\")\n",
    "val frames = List(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "loose-chapter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zip: List[(String, Int)] = List((a,1), (b,2), (c,3))\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val zip = cards.zip(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "harmful-watts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "1\n",
      "b\n",
      "2\n",
      "c\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "for ( (stringy, inty) <- zip) {\n",
    "    println(stringy)\n",
    "    println(inty)\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel [conda env:spark]",
   "language": "scala",
   "name": "conda-env-spark-spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
