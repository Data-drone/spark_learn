{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c9e4f15-ca9c-41df-bcaa-1c4acdf60afc",
   "metadata": {},
   "source": [
    "# Optimize Spark Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99cbb8-a1ea-4d2f-85b6-0ee26a8f1782",
   "metadata": {},
   "source": [
    "Explore the table structure in Minio and optimise the file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5353f20-cb12-470f-b4cb-d4e1d35a055a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# load spark session templates\n",
    "from spark_utils import get_k8s_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a558324-28b8-4400-b59a-6f7ad235f59b",
   "metadata": {},
   "source": [
    "# Initialise Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ec6cc4-b4fb-474a-b742-a440970ec3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_SUBMIT_ARGS = (\"--jars local:///opt/spark-jars/hadoop-aws-3.2.0.jar,\"\n",
    "                     \"local:///opt/spark-jars/delta-core_2.12-1.0.0.jar,\"\n",
    "                     \"local:///opt/spark-jars/aws-java-sdk-bundle-1.11.375.jar,\"\n",
    "                     \"local:///opt/sparkRapidsPlugin/cudf-21.08.2-cuda11.jar,\"\n",
    "                     \"local:///opt/sparkRapidsPlugin/rapids-4-spark_2.12-21.08.0.jar\"\n",
    "                     \" pyspark-shell\")\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = BASIC_SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1621dec5-62ca-48d2-93a0-265e50c6725f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 13:47:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/03 13:47:40 WARN ResourceUtils: The configuration of cores (exec = 4 task = 1, runnable tasks = 4) will result in wasted resources due to resource gpu limiting the number of runnable tasks per executor to: 1. Please adjust your configuration.\n",
      "21/10/03 13:47:44 WARN SQLExecPlugin: RAPIDS Accelerator 21.08.0 using cudf 21.08.2. To disable GPU support set `spark.rapids.sql.enabled` to false\n",
      "21/10/03 13:47:44 WARN Plugin: Installing rapids UDF compiler extensions to Spark. The compiler is disabled by default. To enable it, set `spark.rapids.sql.udfCompiler.enabled` to true\n"
     ]
    }
   ],
   "source": [
    "access_key = 'AKIAIOSFODNN7EXAMPLE' # os.environ['MINIO_ACCESS_KEY']\n",
    "secret_key = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' # os.environ['MINIO_SECRET_KEY']\n",
    "\n",
    "# This cannot be triggered in the python code as the JVM will be activated when it hits the python builder starts\n",
    "# .config(\"spark.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\")\n",
    "\n",
    "# debug\n",
    "# .config(\"spark.kubernetes.executor.deleteOnTermination\", \"false\")\n",
    "\n",
    "spark = (get_k8s_spark()\n",
    "            .config(\"spark.kubernetes.container.image\", \n",
    "                    \"k3d-test-registry:5000/datadrone/k8s-spark-worker:3.1.2-hadoop3.2-rapids-k8s\")\n",
    "            .config(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", access_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio-tenant.svc.cluster.local\")\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "            .config(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "            .config(\"spark.task.resource.gpu.amount\", \"1\")\n",
    "            .config(\"spark.driver.cores\", \"4\")   \n",
    "            .config(\"spark.driver.memory\", \"8g\")\n",
    "            .config(\"spark.executor.cores\", \"4\")\n",
    "            .config(\"spark.num.executors\", 2)\n",
    "            .config(\"spark.executor.memory\", \"10g\")\n",
    "            .config(\"spark.executor.resource.gpu.discoveryScript\", \"/opt/sparkRapidsPlugin/getGpusResources.sh\")\n",
    "            .config(\"spark.executor.resource.gpu.vendor\", \"nvidia.com\")\n",
    "            .config(\"spark.rapids.memory.pinnedPool.size\", \"2G\")\n",
    "            .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "            .config(\"spark.rapids.sql.concurrentGpuTasks\", \"2\")\n",
    "            .config(\"spark.rapids.sql.udfCompiler.enabled\", True)\n",
    "            .appName(\"Spark K8s\")\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c071e02-835a-4745-bea1-ebc6f8666d9f",
   "metadata": {},
   "source": [
    "# Get the table stats with Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af9192-72dc-480e-a2e3-3b065a2326b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd6318-00e1-49ae-9000-6b6f373766c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "k8s_minio_client = Minio(\n",
    "        \"minio.minio-tenant.svc.cluster.local\",\n",
    "        access_key='AKIAIOSFODNN7EXAMPLE',\n",
    "        secret_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n",
    "        secure=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cdb8e1-433c-4eb3-98d4-43f901a88d03",
   "metadata": {},
   "source": [
    "# Get Minio zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06328897-d8ea-44c9-98ab-68269bcd199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check on sizes and stuff\n",
    "contents = k8s_minio_client.list_objects('data', \n",
    "                                            recursive=True, \n",
    "                                            prefix='warehouse')\n",
    "\n",
    "obj_names = []\n",
    "obj_length = []\n",
    "for thing in contents:\n",
    "    obj_names.append(thing.object_name)\n",
    "    obj_length.append(thing.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f973d4-2269-4cb4-84c3-3ccdbef1232b",
   "metadata": {},
   "source": [
    "## Raw Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5376e957-ebaa-45b2-9548-e5ccf2b59247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick check on sizes and stuff\n",
    "contents = k8s_minio_client.list_objects('data', \n",
    "                                            recursive=True, \n",
    "                                            prefix='warehouse/raw')\n",
    "\n",
    "obj_names = []\n",
    "obj_length = []\n",
    "for thing in contents:\n",
    "    obj_names.append(thing.object_name)\n",
    "    obj_length.append(thing.size)\n",
    "    \n",
    "data_dict = {'obj_name':obj_names, 'obj_length':obj_length}\n",
    "raw_minio_df = pd.DataFrame(data_dict)\n",
    "df_m1 = raw_minio_df['obj_name'].str.split('/', expand=True)\n",
    "raw_minio_df['root'] = df_m1[0] \n",
    "raw_minio_df['zone'] = df_m1[1]\n",
    "raw_minio_df['table'] = df_m1[2]\n",
    "df_analysis = raw_minio_df.groupby('table').agg({'obj_length':['sum', 'mean', 'count']})\n",
    "\n",
    "df_analysis['obj_length', 'avg_file_mb'] = df_analysis['obj_length', 'sum'] / 1.0e8\n",
    "df_analysis['obj_length', 'num_files'] = df_analysis['obj_length', 'sum'] / 1.28e8\n",
    "df_analysis['obj_length', 'num_files'] = df_analysis['obj_length', 'num_files'].round(0)\n",
    "df_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc1f2cd-0f43-4947-9a41-6c400e8c9e44",
   "metadata": {},
   "source": [
    "# Quick Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae2ed2d-3e91-4a1e-9461-6579819a573c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 13:48:08 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "21/10/03 13:48:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_data = \"s3a://data/warehouse/processed/nyc_taxi_dataset\"\n",
    "\n",
    "delta_data = spark.read.option(\"header\", True).format(\"delta\").load(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2c565a3-7902-410a-a3a9-2f1efc72186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "496002404"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30e0fbc-4d9c-4324-aca0-1f37083bfe19",
   "metadata": {},
   "source": [
    "# Shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025c8f8-4c14-498d-8545-2efcc6bc4cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c044563e-7410-4f50-a5ca-a23177557e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark]",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
