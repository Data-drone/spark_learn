{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e70fbd-60f3-4c6c-b91d-a36871b1bdb5",
   "metadata": {},
   "source": [
    "# Using Spark on Kubernetes\n",
    "\n",
    "This is a testing notebook and also \"cheat sheet\" to make sure everything is running and connecting\n",
    "for my kubernetes spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9143c6a-1948-46ea-bdd3-085b7b472388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42a59fe-6a07-45ab-a48b-ec00f6253833",
   "metadata": {},
   "source": [
    "## Objectstore Tests \n",
    "\n",
    "we are using Minio as our object store so firstly lets test it independent of spark\n",
    "if we return buckets then all is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a84ad64-524c-49a6-8f31-47dc9995c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f43f004-c109-41f3-9d17-d573fe7dea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client = Minio(\n",
    "        \"minio.minio-tenant.svc.cluster.local\",\n",
    "        access_key='AKIAIOSFODNN7EXAMPLE',\n",
    "        secret_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n",
    "        secure=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "25254032-5746-4c53-825d-5c7ad505d59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warehouse 2021-09-19 14:11:20.375000+00:00\n"
     ]
    }
   ],
   "source": [
    "buckets = minio_client.list_buckets()\n",
    "\n",
    "for bucket in buckets:\n",
    "    print(bucket.name, bucket.creation_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe81ad-1996-4705-841d-e3495a79ead6",
   "metadata": {},
   "source": [
    "## Configs\n",
    "\n",
    "These configs are set to work with the stack at: https://github.com/Data-drone/data_eng_kube.git\n",
    "\n",
    "Note compared to Spark 2.x, Spark 3.x doesn't properly maven load spark.jars.packages:\n",
    "https://issues.apache.org/jira/browse/SPARK-35084\n",
    "\n",
    "We need to have at least the hadoop-aws jar already on drivers and executors to make things work more smoothly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "db3981c5-c878-4ef7-b0e6-46008eed24fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fe33db846a0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "sparkConf.setAppName(\"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.container.image\", \"k3d-test-registry:5000/datadrone/spark-worker:3.1.2-hadoop3.2-rapids\")\n",
    "sparkConf.set(\"spark.kubernetes.namespace\", \"jhub\")\n",
    "sparkConf.set(\"spark.executor.instances\", \"7\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"2\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"512m\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"512m\")\n",
    "sparkConf.set(\"spark.kubernetes.pyspark.pythonVersion\", \"3\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark\")\n",
    "\n",
    "### Adding minio settings\n",
    "# need to add jars: org.apache.hadoop:hadoop-aws:3.2.0\n",
    "sparkConf.set(\"spark.jars.packages\", [\"org.apache.hadoop:hadoop-aws:3.2.0\"])\n",
    "\n",
    "access_key = 'AKIAIOSFODNN7EXAMPLE' # os.environ['MINIO_ACCESS_KEY']\n",
    "secret_key = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' # os.environ['MINIO_SECRET_KEY']\n",
    "\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.access.key\", access_key)\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.secret.key\", secret_key)\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio-tenant.svc.cluster.local\")\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            \n",
    "#sparkConf.set(\"spark.driver.blockManager.port\", \"7777\")\n",
    "#sparkConf.set(\"spark.driver.port\", \"2222\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2bf78f68-c303-4d94-8a7c-1bd307fe30e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f5287df9-0940-43c2-9411-a10d5869d5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"kubernetes-dispatcher-0\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.util.Arrays.copyOf(Arrays.java:3181)\n",
      "\tat com.fasterxml.jackson.core.sym.ByteQuadsCanonicalizer._verifySharing(ByteQuadsCanonicalizer.java:863)\n",
      "\tat com.fasterxml.jackson.core.sym.ByteQuadsCanonicalizer.addName(ByteQuadsCanonicalizer.java:811)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.addName(UTF8StreamJsonParser.java:2355)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.parseEscapedName(UTF8StreamJsonParser.java:2000)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.parseName(UTF8StreamJsonParser.java:1895)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._parseName(UTF8StreamJsonParser.java:1712)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextFieldName(UTF8StreamJsonParser.java:1029)\n",
      "\tat com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:250)\n",
      "\tat com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:258)\n",
      "\tat com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:258)\n",
      "\tat com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:258)\n",
      "\tat com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:258)\n",
      "\tat com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:258)\n",
      "\tat com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeArray(JsonNodeDeserializer.java:437)\n",
      "\tat com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:261)\n",
      "\tat com.fasterxml.jackson.databind.deser.std.BaseNodeDeserializer.deserializeObject(JsonNodeDeserializer.java:258)\n",
      "\tat com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:68)\n",
      "\tat com.fasterxml.jackson.databind.deser.std.JsonNodeDeserializer.deserialize(JsonNodeDeserializer.java:15)\n",
      "\tat com.fasterxml.jackson.databind.ObjectMapper._readValue(ObjectMapper.java:4173)\n",
      "\tat com.fasterxml.jackson.databind.ObjectMapper.readTree(ObjectMapper.java:2577)\n",
      "\tat com.fasterxml.jackson.core.JsonParser.readValueAsTree(JsonParser.java:1818)\n",
      "\tat io.fabric8.kubernetes.internal.KubernetesDeserializer.deserialize(KubernetesDeserializer.java:55)\n",
      "\tat io.fabric8.kubernetes.internal.KubernetesDeserializer.deserialize(KubernetesDeserializer.java:46)\n",
      "\tat com.fasterxml.jackson.databind.deser.impl.MethodProperty.deserializeAndSet(MethodProperty.java:129)\n",
      "\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.vanillaDeserialize(BeanDeserializer.java:288)\n",
      "\tat com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:151)\n",
      "\tat com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4202)\n",
      "\tat com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:3250)\n",
      "\tat io.fabric8.kubernetes.client.utils.Serialization.unmarshal(Serialization.java:247)\n",
      "\tat io.fabric8.kubernetes.client.utils.Serialization.unmarshal(Serialization.java:168)\n",
      "\tat io.fabric8.kubernetes.client.utils.Serialization.unmarshal(Serialization.java:153)\n"
     ]
    }
   ],
   "source": [
    "# check loaded jars\n",
    "print(spark.sparkContext._jsc.sc().listJars())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4a969-bfa3-4099-8290-3bb3df7532b7",
   "metadata": {},
   "source": [
    "# Generate some test data and run through Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "256995b0-71c2-46a8-9d32-bc59a07ba33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba0333e3-c9fe-44cb-9dc3-5002df62e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(100000,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8d32d31-b7e2-4c57-8071-6ddb66c84b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.272922</td>\n",
       "      <td>-0.744408</td>\n",
       "      <td>0.241597</td>\n",
       "      <td>-0.802008</td>\n",
       "      <td>1.832615</td>\n",
       "      <td>1.161991</td>\n",
       "      <td>-0.719084</td>\n",
       "      <td>-0.156190</td>\n",
       "      <td>1.298619</td>\n",
       "      <td>-0.515676</td>\n",
       "      <td>-0.301739</td>\n",
       "      <td>-0.134644</td>\n",
       "      <td>-1.266090</td>\n",
       "      <td>-0.717008</td>\n",
       "      <td>-0.418787</td>\n",
       "      <td>-1.188823</td>\n",
       "      <td>1.077287</td>\n",
       "      <td>1.495263</td>\n",
       "      <td>-0.401542</td>\n",
       "      <td>0.544466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.718030</td>\n",
       "      <td>0.417820</td>\n",
       "      <td>-1.147456</td>\n",
       "      <td>-0.982689</td>\n",
       "      <td>-0.564373</td>\n",
       "      <td>0.393020</td>\n",
       "      <td>-0.676361</td>\n",
       "      <td>-0.120340</td>\n",
       "      <td>-2.160900</td>\n",
       "      <td>-0.188294</td>\n",
       "      <td>-0.089786</td>\n",
       "      <td>-1.341964</td>\n",
       "      <td>2.064482</td>\n",
       "      <td>0.199910</td>\n",
       "      <td>-0.600701</td>\n",
       "      <td>0.348775</td>\n",
       "      <td>-1.786512</td>\n",
       "      <td>0.074005</td>\n",
       "      <td>-0.741564</td>\n",
       "      <td>-1.006054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.166078</td>\n",
       "      <td>0.545512</td>\n",
       "      <td>-1.892233</td>\n",
       "      <td>-0.688208</td>\n",
       "      <td>0.431705</td>\n",
       "      <td>-0.372382</td>\n",
       "      <td>-0.779623</td>\n",
       "      <td>-0.431989</td>\n",
       "      <td>2.215195</td>\n",
       "      <td>0.561898</td>\n",
       "      <td>-1.241156</td>\n",
       "      <td>0.445456</td>\n",
       "      <td>-0.664669</td>\n",
       "      <td>0.135789</td>\n",
       "      <td>0.695837</td>\n",
       "      <td>0.327513</td>\n",
       "      <td>-0.677144</td>\n",
       "      <td>-0.043095</td>\n",
       "      <td>2.130636</td>\n",
       "      <td>-0.902576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.533192</td>\n",
       "      <td>0.654566</td>\n",
       "      <td>0.455884</td>\n",
       "      <td>-0.392624</td>\n",
       "      <td>-0.477823</td>\n",
       "      <td>-1.142813</td>\n",
       "      <td>-0.279524</td>\n",
       "      <td>0.331695</td>\n",
       "      <td>0.979211</td>\n",
       "      <td>-0.456323</td>\n",
       "      <td>0.530391</td>\n",
       "      <td>0.669734</td>\n",
       "      <td>1.106799</td>\n",
       "      <td>-2.149109</td>\n",
       "      <td>-1.666105</td>\n",
       "      <td>0.304300</td>\n",
       "      <td>-0.121841</td>\n",
       "      <td>0.277793</td>\n",
       "      <td>-0.405168</td>\n",
       "      <td>1.306310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.668425</td>\n",
       "      <td>-1.228909</td>\n",
       "      <td>-1.454736</td>\n",
       "      <td>-0.704847</td>\n",
       "      <td>-0.419576</td>\n",
       "      <td>-0.791204</td>\n",
       "      <td>0.982277</td>\n",
       "      <td>0.345302</td>\n",
       "      <td>-0.619585</td>\n",
       "      <td>0.455096</td>\n",
       "      <td>-0.458011</td>\n",
       "      <td>1.609000</td>\n",
       "      <td>0.796319</td>\n",
       "      <td>-0.199821</td>\n",
       "      <td>1.228253</td>\n",
       "      <td>-0.074364</td>\n",
       "      <td>-0.150461</td>\n",
       "      <td>1.839072</td>\n",
       "      <td>-1.380323</td>\n",
       "      <td>1.388740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.272922 -0.744408  0.241597 -0.802008  1.832615  1.161991 -0.719084   \n",
       "1  0.718030  0.417820 -1.147456 -0.982689 -0.564373  0.393020 -0.676361   \n",
       "2  0.166078  0.545512 -1.892233 -0.688208  0.431705 -0.372382 -0.779623   \n",
       "3  0.533192  0.654566  0.455884 -0.392624 -0.477823 -1.142813 -0.279524   \n",
       "4  0.668425 -1.228909 -1.454736 -0.704847 -0.419576 -0.791204  0.982277   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0 -0.156190  1.298619 -0.515676 -0.301739 -0.134644 -1.266090 -0.717008   \n",
       "1 -0.120340 -2.160900 -0.188294 -0.089786 -1.341964  2.064482  0.199910   \n",
       "2 -0.431989  2.215195  0.561898 -1.241156  0.445456 -0.664669  0.135789   \n",
       "3  0.331695  0.979211 -0.456323  0.530391  0.669734  1.106799 -2.149109   \n",
       "4  0.345302 -0.619585  0.455096 -0.458011  1.609000  0.796319 -0.199821   \n",
       "\n",
       "         14        15        16        17        18        19  \n",
       "0 -0.418787 -1.188823  1.077287  1.495263 -0.401542  0.544466  \n",
       "1 -0.600701  0.348775 -1.786512  0.074005 -0.741564 -1.006054  \n",
       "2  0.695837  0.327513 -0.677144 -0.043095  2.130636 -0.902576  \n",
       "3 -1.666105  0.304300 -0.121841  0.277793 -0.405168  1.306310  \n",
       "4  1.228253 -0.074364 -0.150461  1.839072 -1.380323  1.388740  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7fd3dfb-9a5e-431d-8299-8d4058ee840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF=spark.createDataFrame(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e385c728-2580-40fb-a253-ca4e54c9b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 0: double (nullable = true)\n",
      " |-- 1: double (nullable = true)\n",
      " |-- 2: double (nullable = true)\n",
      " |-- 3: double (nullable = true)\n",
      " |-- 4: double (nullable = true)\n",
      " |-- 5: double (nullable = true)\n",
      " |-- 6: double (nullable = true)\n",
      " |-- 7: double (nullable = true)\n",
      " |-- 8: double (nullable = true)\n",
      " |-- 9: double (nullable = true)\n",
      " |-- 10: double (nullable = true)\n",
      " |-- 11: double (nullable = true)\n",
      " |-- 12: double (nullable = true)\n",
      " |-- 13: double (nullable = true)\n",
      " |-- 14: double (nullable = true)\n",
      " |-- 15: double (nullable = true)\n",
      " |-- 16: double (nullable = true)\n",
      " |-- 17: double (nullable = true)\n",
      " |-- 18: double (nullable = true)\n",
      " |-- 19: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a1a3e-518d-454e-ae54-ded860dfd3cf",
   "metadata": {},
   "source": [
    "# Load Data and write it to my object store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "25e00de5-af43-48e2-9355-29076cbbaeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly create a new bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3825d419-485b-40a6-a3b3-6fe4ebc40ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    minio_client.make_bucket('testing-bucket')\n",
    "except ResponseError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ab5afa5d-d0b8-4bc0-b590-03c8110f3427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.18.48-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.22.0,>=1.21.48\n",
      "  Downloading botocore-1.21.48-py3-none-any.whl (7.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 7.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 2.5 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.22.0,>=1.21.48->boto3) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.22.0,>=1.21.48->boto3) (1.26.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.48->boto3) (1.15.0)\n",
      "\u001b[33mWARNING: Error parsing requirements for numpy: [Errno 2] No such file or directory: '/opt/conda/lib/python3.9/site-packages/numpy-1.21.2.dist-info/METADATA'\u001b[0m\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "Successfully installed boto3-1.18.48 botocore-1.21.48 jmespath-0.10.0 s3transfer-0.5.0\n"
     ]
    }
   ],
   "source": [
    "# need boto to pull from AWS\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef5e4c8b-9db9-4094-b90d-6b09f3628233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1f891cb-7a57-4ace-b9f6-39dad1cc5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a82020a-8917-4ab0-972c-7c0609aef79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "output_bucket = 'testing-bucket'\n",
    "testing_file = 'green_tripdata_2015-07.csv'\n",
    "load_path = 'trip data/' + testing_file\n",
    "write_path = 'raw_data/' + testing_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3965471-43ef-4ccd-8c0e-b2a86c040f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('green_tripdata_2015-07.csv', 'wb') as f:\n",
    "        s3.download_fileobj('nyc-tlc', load_path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "743e8d4d-3100-4479-9888-c2e2fbe23828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minio.helpers.ObjectWriteResult at 0x7fe34eb2f190>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minio_client.fput_object(output_bucket, write_path, testing_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3445c5e1-9abe-4e9f-8ce0-ab1ca2770b01",
   "metadata": {},
   "source": [
    "## Reading the loaded Data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "95d5879b-a8bb-4134-8735-d8447cd049f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "faaed990-b396-45b8-acbe-06253067fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9ffe7a62-0c48-4628-885e-6365fbc42944",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o455.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:795)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_110/3385948945.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3a://'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutput_bucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrite_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o455.csv.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2595)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:361)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:46)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:377)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:325)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:307)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:307)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:795)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2499)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593)\n\t... 25 more\n"
     ]
    }
   ],
   "source": [
    "raw_data = spark.read.option(\"header\", True).csv(os.path.join('s3a://' + output_bucket, write_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9300dfa4-24c9-41fa-a274-e52f8f546670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/26 06:03:06 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n"
     ]
    }
   ],
   "source": [
    "# Shutdown Our Context\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067f3f7-eb90-4371-9ee3-35170197d695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
