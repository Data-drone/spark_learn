{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e70fbd-60f3-4c6c-b91d-a36871b1bdb5",
   "metadata": {},
   "source": [
    "# Using Spark on Kubernetes\n",
    "\n",
    "This is a testing notebook and also \"cheat sheet\" to make sure everything is running and connecting\n",
    "for my kubernetes spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9143c6a-1948-46ea-bdd3-085b7b472388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import socket # to get the internal ipaddress for setting the spark driver\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42a59fe-6a07-45ab-a48b-ec00f6253833",
   "metadata": {},
   "source": [
    "## Objectstore Tests \n",
    "\n",
    "we are using Minio as our object store so firstly lets test it independent of spark\n",
    "if we return buckets then all is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a84ad64-524c-49a6-8f31-47dc9995c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f43f004-c109-41f3-9d17-d573fe7dea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client = Minio(\n",
    "        \"minio.minio-tenant.svc.cluster.local\",\n",
    "        access_key='AKIAIOSFODNN7EXAMPLE',\n",
    "        secret_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n",
    "        secure=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25254032-5746-4c53-825d-5c7ad505d59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing-bucket 2021-09-26 06:04:35.273000+00:00\n",
      "warehouse 2021-09-19 14:11:20.375000+00:00\n"
     ]
    }
   ],
   "source": [
    "buckets = minio_client.list_buckets()\n",
    "\n",
    "for bucket in buckets:\n",
    "    print(bucket.name, bucket.creation_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe81ad-1996-4705-841d-e3495a79ead6",
   "metadata": {},
   "source": [
    "## Configs\n",
    "\n",
    "These configs are set to work with the stack at: https://github.com/Data-drone/data_eng_kube.git\n",
    "\n",
    "Note compared to Spark 2.x, Spark 3.x doesn't properly maven load spark.jars.packages:\n",
    "https://issues.apache.org/jira/browse/SPARK-35084\n",
    "\n",
    "We need to have at least the hadoop-aws jar already on drivers and executors to make things work more smoothly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec8f680-0e54-4ac5-b000-0b8638a1fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_ARGS = \"--packages org.apache.hadoop:hadoop-aws:3.2.0 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db3981c5-c878-4ef7-b0e6-46008eed24fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f7d083823d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "sparkConf.setAppName(\"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.container.image\", \"k3d-test-registry:5000/datadrone/spark-test2:latest\")\n",
    "sparkConf.set(\"spark.kubernetes.namespace\", \"jhub\")\n",
    "sparkConf.set(\"spark.executor.instances\", \"2\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"4\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"1g\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"512m\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"512m\")\n",
    "sparkConf.set(\"spark.pyspark.python\", \"/opt/conda/bin/python\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark\")\n",
    "\n",
    "### Adding minio settings\n",
    "# need to add jars: org.apache.hadoop:hadoop-aws:3.2.0\n",
    "sparkConf.set(\"spark.jars.packages\", [\"org.apache.hadoop:hadoop-aws:3.2.0\"])\n",
    "#sparkConf.set(\"spark.jars.ivy\", \"/opt/\")\n",
    "\n",
    "access_key = 'AKIAIOSFODNN7EXAMPLE' # os.environ['MINIO_ACCESS_KEY']\n",
    "secret_key = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' # os.environ['MINIO_SECRET_KEY']\n",
    "\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.access.key\", access_key)\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.secret.key\", secret_key)\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio-tenant.svc.cluster.local\")\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            \n",
    "#sparkConf.set(\"spark.driver.blockManager.port\", \"7777\")\n",
    "#sparkConf.set(\"spark.driver.port\", \"2222\")\n",
    "\n",
    "# we needed to set the ip address for the host for some reason...\n",
    "sparkConf.set(\"spark.driver.host\", socket.gethostbyname(socket.gethostname()))\n",
    "sparkConf.set(\"spark.submit.deployMode\", \"client\")\n",
    "\n",
    "sparkConf.set(\"spark.driver.port\", \"7778\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "#sparkConf.set(\"spark.driver.blockManager.port\", \"7777\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bf78f68-c303-4d94-8a7c-1bd307fe30e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/envs/spark/lib/python3.8/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1040ce9b-5991-4c54-b0df-5a7e68e4352a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      ":: resolution report :: resolve 107ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1040ce9b-5991-4c54-b0df-5a7e68e4352a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n",
      "21/09/28 00:40:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5287df9-0940-43c2-9411-a10d5869d5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(spark://10.42.4.186:7778/jars/org.apache.hadoop_hadoop-aws-3.2.0.jar, spark://10.42.4.186:7778/jars/com.amazonaws_aws-java-sdk-bundle-1.11.375.jar)\n"
     ]
    }
   ],
   "source": [
    "# check loaded jars\n",
    "print(spark.sparkContext._jsc.sc().listJars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4f7b761-4d7b-4e9b-aae1-41fe98be05fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate sum: 45.0\n"
     ]
    }
   ],
   "source": [
    "# test spark without reading data\n",
    "# Create a distributed data set to test to the session\n",
    "t = spark.sparkContext.parallelize(range(10))\n",
    "\n",
    "# Calculate the approximate sum of values in the dataset\n",
    "r = t.sumApprox(3)\n",
    "print('Approximate sum: %s' % r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4a969-bfa3-4099-8290-3bb3df7532b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate some test data and run through Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256995b0-71c2-46a8-9d32-bc59a07ba33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0333e3-c9fe-44cb-9dc3-5002df62e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(100000,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d32d31-b7e2-4c57-8071-6ddb66c84b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd3dfb-9a5e-431d-8299-8d4058ee840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF=spark.createDataFrame(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385c728-2580-40fb-a253-ca4e54c9b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a1a3e-518d-454e-ae54-ded860dfd3cf",
   "metadata": {},
   "source": [
    "# Load Data and write it to my object store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e00de5-af43-48e2-9355-29076cbbaeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly create a new bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825d419-485b-40a6-a3b3-6fe4ebc40ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    minio_client.make_bucket('testing-bucket')\n",
    "except ResponseError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5afa5d-d0b8-4bc0-b590-03c8110f3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need boto to pull from AWS\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5e4c8b-9db9-4094-b90d-6b09f3628233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f891cb-7a57-4ace-b9f6-39dad1cc5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a82020a-8917-4ab0-972c-7c0609aef79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "output_bucket = 'testing-bucket'\n",
    "testing_file = 'green_tripdata_2015-07.csv'\n",
    "load_path = 'trip data/' + testing_file\n",
    "write_path = 'raw_data/' + testing_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3965471-43ef-4ccd-8c0e-b2a86c040f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('green_tripdata_2015-07.csv', 'wb') as f:\n",
    "        s3.download_fileobj('nyc-tlc', load_path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e8d4d-3100-4479-9888-c2e2fbe23828",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client.fput_object(output_bucket, write_path, testing_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3445c5e1-9abe-4e9f-8ce0-ab1ca2770b01",
   "metadata": {},
   "source": [
    "## Reading the loaded Data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95d5879b-a8bb-4134-8735-d8447cd049f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152bb85e-7244-4edc-8a6b-e6b98cadc456",
   "metadata": {},
   "source": [
    "Test read from minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ffe7a62-0c48-4628-885e-6365fbc42944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/28 00:42:43 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_data = spark.read.option(\"header\", True).csv(os.path.join('s3a://' + output_bucket, write_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24060e42-2225-4f71-9e51-bddd7152e833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- Lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- Store_and_fwd_flag: string (nullable = true)\n",
      " |-- RateCodeID: string (nullable = true)\n",
      " |-- Pickup_longitude: string (nullable = true)\n",
      " |-- Pickup_latitude: string (nullable = true)\n",
      " |-- Dropoff_longitude: string (nullable = true)\n",
      " |-- Dropoff_latitude: string (nullable = true)\n",
      " |-- Passenger_count: string (nullable = true)\n",
      " |-- Trip_distance: string (nullable = true)\n",
      " |-- Fare_amount: string (nullable = true)\n",
      " |-- Extra: string (nullable = true)\n",
      " |-- MTA_tax: string (nullable = true)\n",
      " |-- Tip_amount: string (nullable = true)\n",
      " |-- Tolls_amount: string (nullable = true)\n",
      " |-- Ehail_fee: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- Total_amount: string (nullable = true)\n",
      " |-- Payment_type: string (nullable = true)\n",
      " |-- Trip_type : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4806803-d786-4318-a175-5ef8099f94b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0ceae63-6bcc-4352-bea2-353107922da5",
   "metadata": {},
   "source": [
    "# Close out Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300dfa4-24c9-41fa-a274-e52f8f546670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Our Context\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067f3f7-eb90-4371-9ee3-35170197d695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark]",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
