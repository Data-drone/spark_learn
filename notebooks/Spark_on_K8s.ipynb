{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e70fbd-60f3-4c6c-b91d-a36871b1bdb5",
   "metadata": {},
   "source": [
    "# Using Spark on Kubernetes\n",
    "\n",
    "This is a testing notebook and also \"cheat sheet\" to make sure everything is running and connecting\n",
    "for my kubernetes spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9143c6a-1948-46ea-bdd3-085b7b472388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import socket # to get the internal ipaddress for setting the spark driver\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42a59fe-6a07-45ab-a48b-ec00f6253833",
   "metadata": {},
   "source": [
    "## Objectstore Tests \n",
    "\n",
    "we are using Minio as our object store so firstly lets test it independent of spark\n",
    "if we return buckets then all is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a84ad64-524c-49a6-8f31-47dc9995c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f43f004-c109-41f3-9d17-d573fe7dea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client = Minio(\n",
    "        \"minio.minio-tenant.svc.cluster.local\",\n",
    "        access_key='AKIAIOSFODNN7EXAMPLE',\n",
    "        secret_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n",
    "        secure=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25254032-5746-4c53-825d-5c7ad505d59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing-bucket 2021-09-26 06:04:35.273000+00:00\n",
      "warehouse 2021-09-19 14:11:20.375000+00:00\n"
     ]
    }
   ],
   "source": [
    "buckets = minio_client.list_buckets()\n",
    "\n",
    "for bucket in buckets:\n",
    "    print(bucket.name, bucket.creation_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe81ad-1996-4705-841d-e3495a79ead6",
   "metadata": {},
   "source": [
    "## Configs\n",
    "\n",
    "These configs are set to work with the stack at: https://github.com/Data-drone/data_eng_kube.git\n",
    "\n",
    "Note compared to Spark 2.x, Spark 3.x doesn't properly maven load spark.jars.packages:\n",
    "https://issues.apache.org/jira/browse/SPARK-35084\n",
    "\n",
    "We need to have at least the hadoop-aws jar already on drivers and executors to make things work more smoothly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec8f680-0e54-4ac5-b000-0b8638a1fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_ARGS = \"--packages org.apache.hadoop:hadoop-aws:3.2.0 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebd85587-acee-44c6-874b-cb34360b8096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHELL=/bin/bash\n",
      "NVIDIA_VISIBLE_DEVICES=all\n",
      "KUBERNETES_SERVICE_PORT_HTTPS=443\n",
      "JUPYTERHUB_ADMIN_ACCESS=1\n",
      "KUBERNETES_SERVICE_PORT=443\n",
      "MINIFORGE_VERSION=4.10.3-3\n",
      "PROXY_API_SERVICE_HOST=10.43.154.6\n",
      "HOSTNAME=jupyter-jovyan\n",
      "LANGUAGE=en_US.UTF-8\n",
      "JUPYTERHUB_API_TOKEN=df54273901df450cbe85788e7543167a\n",
      "NVIDIA_REQUIRE_CUDA=cuda>=11.1 brand=tesla,driver>=418,driver<419 brand=tesla,driver>=440,driver<441 driver>=450\n",
      "PROXY_API_SERVICE_PORT=8001\n",
      "JUPYTERHUB_BASE_URL=/jupyter/\n",
      "NB_UID=1000\n",
      "PROXY_PUBLIC_PORT_80_TCP=tcp://10.43.45.18:80\n",
      "PROXY_PUBLIC_PORT=tcp://10.43.45.18:80\n",
      "PROXY_PUBLIC_SERVICE_PORT_HTTP=80\n",
      "PWD=/home/jovyan/spark_learn/notebooks\n",
      "NVIDIA_DRIVER_CAPABILITIES=compute,utility\n",
      "MEM_GUARANTEE=1073741824\n",
      "JUPYTER_IMAGE=k3d-test-registry:5000/datadrone/spark_notebook_kube\n",
      "PROXY_API_PORT_8001_TCP_ADDR=10.43.154.6\n",
      "PYSPARK_SUBMIT_ARGS=--packages org.apache.hadoop:hadoop-aws:3.2.0 pyspark-shell\n",
      "HUB_SERVICE_HOST=10.43.215.180\n",
      "JUPYTERHUB_SERVER_NAME=\n",
      "HOME=/home/jovyan\n",
      "LANG=en_US.UTF-8\n",
      "KUBERNETES_PORT_443_TCP=tcp://10.43.0.1:443\n",
      "JPY_API_TOKEN=df54273901df450cbe85788e7543167a\n",
      "PROXY_API_PORT_8001_TCP_PORT=8001\n",
      "HUB_SERVICE_PORT=8081\n",
      "CUDA_VERSION=11.1.1\n",
      "NB_GID=100\n",
      "JUPYTERHUB_SERVICE_PREFIX=/jupyter/user/jovyan/\n",
      "PYDEVD_USE_FRAME_EVAL=NO\n",
      "JUPYTERHUB_OAUTH_CALLBACK_URL=/jupyter/user/jovyan/oauth_callback\n",
      "CLICOLOR=1\n",
      "PROXY_PUBLIC_SERVICE_HOST=10.43.45.18\n",
      "PROXY_PUBLIC_PORT_80_TCP_PROTO=tcp\n",
      "HUB_PORT=tcp://10.43.215.180:8081\n",
      "PROXY_PUBLIC_PORT_80_TCP_ADDR=10.43.45.18\n",
      "JUPYTER_IMAGE_SPEC=k3d-test-registry:5000/datadrone/spark_notebook_kube\n",
      "JPY_PARENT_PID=7\n",
      "HUB_PORT_8081_TCP=tcp://10.43.215.180:8081\n",
      "TERM=xterm-color\n",
      "GIT_PAGER=cat\n",
      "PROXY_API_PORT=tcp://10.43.154.6:8001\n",
      "PROXY_API_PORT_8001_TCP_PROTO=tcp\n",
      "SHLVL=0\n",
      "PAGER=cat\n",
      "CONDA_DIR=/opt/conda\n",
      "KUBERNETES_PORT_443_TCP_PROTO=tcp\n",
      "JUPYTERHUB_API_URL=http://hub:8081/jupyter/hub/api\n",
      "JUPYTERHUB_CLIENT_ID=jupyterhub-user-jovyan\n",
      "PROXY_PUBLIC_PORT_80_TCP_PORT=80\n",
      "KUBERNETES_PORT_443_TCP_ADDR=10.43.0.1\n",
      "JUPYTERHUB_HOST=\n",
      "MPLBACKEND=module://matplotlib_inline.backend_inline\n",
      "LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "HUB_SERVICE_PORT_HUB=8081\n",
      "CONDA_VERSION=4.10.3\n",
      "NB_USER=jovyan\n",
      "KUBERNETES_SERVICE_HOST=10.43.0.1\n",
      "LC_ALL=en_US.UTF-8\n",
      "KUBERNETES_PORT=tcp://10.43.0.1:443\n",
      "KUBERNETES_PORT_443_TCP_PORT=443\n",
      "PROXY_API_PORT_8001_TCP=tcp://10.43.154.6:8001\n",
      "PROXY_PUBLIC_SERVICE_PORT=80\n",
      "HUB_PORT_8081_TCP_PORT=8081\n",
      "PATH=/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n",
      "HUB_PORT_8081_TCP_ADDR=10.43.215.180\n",
      "HUB_PORT_8081_TCP_PROTO=tcp\n",
      "JUPYTERHUB_USER=jovyan\n",
      "JUPYTERHUB_ACTIVITY_URL=http://hub:8081/jupyter/hub/api/users/jovyan/activity\n",
      "DEBIAN_FRONTEND=noninteractive\n",
      "_=/usr/bin/printenv\n"
     ]
    }
   ],
   "source": [
    "!printenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "db3981c5-c878-4ef7-b0e6-46008eed24fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f9263c5ccd0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "sparkConf.setAppName(\"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.container.image\", \"k3d-test-registry:5000/datadrone/spark-test-k8s:latest\")\n",
    "sparkConf.set(\"spark.kubernetes.namespace\", \"jhub\")\n",
    "sparkConf.set(\"spark.executor.instances\", \"7\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"2\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"512m\")\n",
    "sparkConf.set(\"spark.executor.memory\", \"512m\")\n",
    "sparkConf.set(\"spark.kubernetes.pyspark.pythonVersion\", \"3\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.serviceAccountName\", \"spark\")\n",
    "\n",
    "### Adding minio settings\n",
    "# need to add jars: org.apache.hadoop:hadoop-aws:3.2.0\n",
    "sparkConf.set(\"spark.jars.packages\", [\"org.apache.hadoop:hadoop-aws:3.2.0\"])\n",
    "#sparkConf.set(\"spark.jars.ivy\", \"/opt/\")\n",
    "\n",
    "access_key = 'AKIAIOSFODNN7EXAMPLE' # os.environ['MINIO_ACCESS_KEY']\n",
    "secret_key = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' # os.environ['MINIO_SECRET_KEY']\n",
    "\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.access.key\", access_key)\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.secret.key\", secret_key)\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio-tenant.svc.cluster.local\")\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "sparkConf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            \n",
    "#sparkConf.set(\"spark.driver.blockManager.port\", \"7777\")\n",
    "#sparkConf.set(\"spark.driver.port\", \"2222\")\n",
    "\n",
    "# we needed to set the ip address for the host for some reason...\n",
    "sparkConf.set(\"spark.driver.host\", socket.gethostbyname(socket.gethostname()))\n",
    "sparkConf.set(\"spark.submit.deployMode\", \"client\")\n",
    "\n",
    "sparkConf.set(\"spark.driver.port\", \"7778\")\n",
    "sparkConf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n",
    "#sparkConf.set(\"spark.driver.blockManager.port\", \"7777\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bf78f68-c303-4d94-8a7c-1bd307fe30e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5287df9-0940-43c2-9411-a10d5869d5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector(spark://10.42.4.21:7778/jars/org.apache.hadoop_hadoop-aws-3.2.0.jar, spark://10.42.4.21:7778/jars/com.amazonaws_aws-java-sdk-bundle-1.11.375.jar)\n"
     ]
    }
   ],
   "source": [
    "# check loaded jars\n",
    "print(spark.sparkContext._jsc.sc().listJars())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4a969-bfa3-4099-8290-3bb3df7532b7",
   "metadata": {},
   "source": [
    "# Generate some test data and run through Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256995b0-71c2-46a8-9d32-bc59a07ba33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0333e3-c9fe-44cb-9dc3-5002df62e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(100000,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d32d31-b7e2-4c57-8071-6ddb66c84b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd3dfb-9a5e-431d-8299-8d4058ee840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF=spark.createDataFrame(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385c728-2580-40fb-a253-ca4e54c9b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a1a3e-518d-454e-ae54-ded860dfd3cf",
   "metadata": {},
   "source": [
    "# Load Data and write it to my object store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e00de5-af43-48e2-9355-29076cbbaeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly create a new bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825d419-485b-40a6-a3b3-6fe4ebc40ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    minio_client.make_bucket('testing-bucket')\n",
    "except ResponseError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5afa5d-d0b8-4bc0-b590-03c8110f3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need boto to pull from AWS\n",
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5e4c8b-9db9-4094-b90d-6b09f3628233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f891cb-7a57-4ace-b9f6-39dad1cc5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a82020a-8917-4ab0-972c-7c0609aef79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "output_bucket = 'testing-bucket'\n",
    "testing_file = 'green_tripdata_2015-07.csv'\n",
    "load_path = 'trip data/' + testing_file\n",
    "write_path = 'raw_data/' + testing_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3965471-43ef-4ccd-8c0e-b2a86c040f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('green_tripdata_2015-07.csv', 'wb') as f:\n",
    "        s3.download_fileobj('nyc-tlc', load_path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e8d4d-3100-4479-9888-c2e2fbe23828",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client.fput_object(output_bucket, write_path, testing_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3445c5e1-9abe-4e9f-8ce0-ab1ca2770b01",
   "metadata": {},
   "source": [
    "## Reading the loaded Data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95d5879b-a8bb-4134-8735-d8447cd049f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe7a62-0c48-4628-885e-6365fbc42944",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "raw_data = spark.read.option(\"header\", True).csv(os.path.join('s3a://' + output_bucket, write_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24060e42-2225-4f71-9e51-bddd7152e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4806803-d786-4318-a175-5ef8099f94b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0ceae63-6bcc-4352-bea2-353107922da5",
   "metadata": {},
   "source": [
    "# Close out Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9300dfa4-24c9-41fa-a274-e52f8f546670",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/27 13:32:19 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n"
     ]
    }
   ],
   "source": [
    "# Shutdown Our Context\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067f3f7-eb90-4371-9ee3-35170197d695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
