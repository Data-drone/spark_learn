{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e70fbd-60f3-4c6c-b91d-a36871b1bdb5",
   "metadata": {},
   "source": [
    "# Using Spark on Kubernetes\n",
    "\n",
    "This is a testing notebook and also \"cheat sheet\" to make sure everything is running and connecting\n",
    "for my kubernetes spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9143c6a-1948-46ea-bdd3-085b7b472388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# load spark session templates\n",
    "from spark_utils import get_k8s_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42a59fe-6a07-45ab-a48b-ec00f6253833",
   "metadata": {},
   "source": [
    "## Objectstore Tests \n",
    "\n",
    "we are using Minio as our object store so firstly lets test it independent of spark\n",
    "if we return buckets then all is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a84ad64-524c-49a6-8f31-47dc9995c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43f004-c109-41f3-9d17-d573fe7dea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client = Minio(\n",
    "        \"minio.minio-tenant.svc.cluster.local\",\n",
    "        access_key='AKIAIOSFODNN7EXAMPLE',\n",
    "        secret_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n",
    "        secure=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25254032-5746-4c53-825d-5c7ad505d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = minio_client.list_buckets()\n",
    "\n",
    "for bucket in buckets:\n",
    "    print(bucket.name, bucket.creation_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe81ad-1996-4705-841d-e3495a79ead6",
   "metadata": {},
   "source": [
    "## Configs\n",
    "\n",
    "These configs are set to work with the stack at: https://github.com/Data-drone/data_eng_kube.git\n",
    "\n",
    "Note compared to Spark 2.x, Spark 3.x doesn't properly maven load spark.jars.packages:\n",
    "https://issues.apache.org/jira/browse/SPARK-35084\n",
    "\n",
    "We need to have at least the hadoop-aws jar already on drivers and executors to make things work more smoothly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1faa7a-1c21-4caf-b2da-927b8d553dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#// https://mvnrepository.com/artifact/com.amazonaws/aws-java-sdk\n",
    "#libraryDependencies += \"com.amazonaws\" % \"aws-java-sdk\" % \"1.12.79\"\n",
    "\n",
    "#\"com.amazonaws:aws-java-sdk:1.12.79\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8f680-0e54-4ac5-b000-0b8638a1fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_ARGS = \"--packages org.apache.hadoop:hadoop-aws:3.2.0,com.amazonaws:aws-java-sdk:1.12.79 --jars {0} \\\n",
    "--driver-class-path {1} pyspark-shell\".format(package_list, classPath)\n",
    "\n",
    "BASIC_SUBMIT_ARGS = \"--packages org.apache.hadoop:hadoop-aws:3.2.0 pyspark-shell\"\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = BASIC_SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed533883-3548-4047-9244-b427f9e12673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "access_key = 'AKIAIOSFODNN7EXAMPLE' # os.environ['MINIO_ACCESS_KEY']\n",
    "secret_key = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' # os.environ['MINIO_SECRET_KEY']\n",
    "\n",
    "spark = (get_k8s_spark()\n",
    "            .config(\"spark.kubernetes.container.image\", \n",
    "                    \"k3d-test-registry:5000/datadrone/k8s-spark-worker:3.1.2-hadoop3.2-rapids-k8s-basic\")\n",
    "            .config(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", access_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio-tenant.svc.cluster.local\")\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            .config(\"spark.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\")\n",
    "            .appName(\"Spark K8s\")\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3981c5-c878-4ef7-b0e6-46008eed24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to add jars: org.apache.hadoop:hadoop-aws:3.2.0\n",
    "#sparkConf.set(\"spark.jars.packages\", [\"org.apache.hadoop:hadoop-aws:3.2.0\"])\n",
    "#sparkConf.set(\"spark.jars.ivy\", \"/opt/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5287df9-0940-43c2-9411-a10d5869d5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check loaded jars\n",
    "print(spark.sparkContext._jsc.sc().listJars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7b761-4d7b-4e9b-aae1-41fe98be05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test spark without reading data\n",
    "# Create a distributed data set to test to the session\n",
    "t = spark.sparkContext.parallelize(range(10))\n",
    "\n",
    "# Calculate the approximate sum of values in the dataset\n",
    "r = t.sumApprox(3)\n",
    "print('Approximate sum: %s' % r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4a969-bfa3-4099-8290-3bb3df7532b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate some test data and run through Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256995b0-71c2-46a8-9d32-bc59a07ba33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0333e3-c9fe-44cb-9dc3-5002df62e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(100000,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d32d31-b7e2-4c57-8071-6ddb66c84b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd3dfb-9a5e-431d-8299-8d4058ee840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF=spark.createDataFrame(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385c728-2580-40fb-a253-ca4e54c9b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a1a3e-518d-454e-ae54-ded860dfd3cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data and write it to my object store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e00de5-af43-48e2-9355-29076cbbaeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly create a new bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825d419-485b-40a6-a3b3-6fe4ebc40ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    minio_client.make_bucket('testing-bucket')\n",
    "except S3Error as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5afa5d-d0b8-4bc0-b590-03c8110f3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need boto to pull from AWS\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5e4c8b-9db9-4094-b90d-6b09f3628233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f891cb-7a57-4ace-b9f6-39dad1cc5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82020a-8917-4ab0-972c-7c0609aef79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "output_bucket = 'testing-bucket'\n",
    "testing_file = 'green_tripdata_2015-07.csv'\n",
    "load_path = 'trip data/' + testing_file\n",
    "write_path = 'raw_data/' + testing_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3965471-43ef-4ccd-8c0e-b2a86c040f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('green_tripdata_2015-07.csv', 'wb') as f:\n",
    "        s3.download_fileobj('nyc-tlc', load_path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e8d4d-3100-4479-9888-c2e2fbe23828",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client.fput_object(output_bucket, write_path, testing_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3445c5e1-9abe-4e9f-8ce0-ab1ca2770b01",
   "metadata": {},
   "source": [
    "## Reading the loaded Data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5879b-a8bb-4134-8735-d8447cd049f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152bb85e-7244-4edc-8a6b-e6b98cadc456",
   "metadata": {},
   "source": [
    "Test read from minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe7a62-0c48-4628-885e-6365fbc42944",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = spark.read.option(\"header\", True).csv(os.path.join('s3a://data/raw_data/green_tripdata_2014-09.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24060e42-2225-4f71-9e51-bddd7152e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4806803-d786-4318-a175-5ef8099f94b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44543f-ed98-44ed-a45a-8431c2c7c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ceae63-6bcc-4352-bea2-353107922da5",
   "metadata": {},
   "source": [
    "# Close out Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300dfa4-24c9-41fa-a274-e52f8f546670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Our Context\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f3c3f1-8f94-4962-9231-b7176d23f776",
   "metadata": {},
   "source": [
    "# Testing Different Class Paths and loading downloaded libs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08daab-690c-42fa-9abc-4c251c7ef8f8",
   "metadata": {},
   "source": [
    "For larger libs we want to have them downloaded already and it would be good to be able to load libs from s3a paths so that we don't have to load workers and driver through local files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b458c97-b0bb-4d7c-91dc-6d9a6909f8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_jarpath = \"s3a://spark-jars/spark-jars/\"\n",
    "local_jarpath = \"/opt/spark-jars/\"\n",
    "\n",
    "package_list = \"{1}hadoop-aws-3.2.0.jar,{1}delta-core_2.12-1.0.0.jar,\\\n",
    "{0}rapids-4-spark-2.12-21.08.0.jar,{0}cudf-21.08.2-cuda11.jar\".format(s3_jarpath, local_jarpath)\n",
    "\n",
    "classPath = \"{1}hadoop-aws-3.2.0.jar:{1}delta-core_2.12-1.0.0.jar:\\\n",
    "{0}rapids-4-spark-2.12-21.08.0.jar:{0}cudf-21.08.2-cuda11.jar\".format(s3_jarpath, local_jarpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9067f3f7-eb90-4371-9ee3-35170197d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOLOAD_SUBMIT_ARGS = \"--packages org.apache.hadoop:hadoop-aws:3.2.0 pyspark-shell\"\n",
    "\n",
    "\"s3a://spark-jars/spark-jars/hadoop-aws-3.2.0.jar\"\n",
    "\"--packages org.apache.hadoop:hadoop-aws:3.2.0 \"\n",
    "\n",
    "BASIC_SUBMIT_ARGS = (\n",
    "                     \"--jars local:///opt/spark-jars/hadoop-aws-3.2.0.jar,\"\n",
    "                     \"local:///opt/spark-jars/delta-core_2.12-1.0.0.jar\"\n",
    "                     \" pyspark-shell\")\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = AUTOLOAD_SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deac0aad-9e45-4bb8-b29f-0cd4bc94f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.9/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-351883e2-6825-4437-a613-991a64e09c8c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.2.0 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      ":: resolution report :: resolve 96ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.2.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-351883e2-6825-4437-a613-991a64e09c8c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/2ms)\n",
      "21/10/03 05:09:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "access_key = 'AKIAIOSFODNN7EXAMPLE' # os.environ['MINIO_ACCESS_KEY']\n",
    "secret_key = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' # os.environ['MINIO_SECRET_KEY']\n",
    "\n",
    "# .config(\"spark.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\")\n",
    "spark = (get_k8s_spark()\n",
    "            .config(\"spark.kubernetes.container.image\", \n",
    "                    \"k3d-test-registry:5000/datadrone/k8s-spark-worker:3.1.2-hadoop3.2-rapids-k8s-basic\")\n",
    "            .config(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", access_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio-tenant.svc.cluster.local\")\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            .config(\"spark.kubernetes.executor.deleteOnTermination\", \"false\")\n",
    "            .appName(\"Spark K8s\")\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c8842a-2a67-4323-a909-a263525732d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 05:09:38 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_data = spark.read.option(\"header\", True).csv(os.path.join('s3a://data/raw_data/green_tripdata_2014-09.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5721e83-ddae-4cde-a4d7-5a0f384756ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- Lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- Store_and_fwd_flag: string (nullable = true)\n",
      " |-- RateCodeID: string (nullable = true)\n",
      " |-- Pickup_longitude: string (nullable = true)\n",
      " |-- Pickup_latitude: string (nullable = true)\n",
      " |-- Dropoff_longitude: string (nullable = true)\n",
      " |-- Dropoff_latitude: string (nullable = true)\n",
      " |-- Passenger_count: string (nullable = true)\n",
      " |-- Trip_distance: string (nullable = true)\n",
      " |-- Fare_amount: string (nullable = true)\n",
      " |-- Extra: string (nullable = true)\n",
      " |-- MTA_tax: string (nullable = true)\n",
      " |-- Tip_amount: string (nullable = true)\n",
      " |-- Tolls_amount: string (nullable = true)\n",
      " |-- Ehail_fee: string (nullable = true)\n",
      " |-- Total_amount: string (nullable = true)\n",
      " |-- Payment_type: string (nullable = true)\n",
      " |-- Trip_type : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec7d45-7844-423c-a270-90bb1c4b60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_warehouse = \"s3a://data/warehouse/raw/green_taxi_pre2015\"\n",
    "\n",
    "delta_data = spark.read.option(\"header\", True).format(\"delta\").load(clean_warehouse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a248f-a177-4208-a8c6-8bfb7cb7e336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Our Context\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e37505-b93f-4e59-8682-4f5828827155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
