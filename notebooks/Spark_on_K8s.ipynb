{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66e70fbd-60f3-4c6c-b91d-a36871b1bdb5",
   "metadata": {},
   "source": [
    "# Using Spark on Kubernetes\n",
    "\n",
    "This is a testing notebook and also \"cheat sheet\" to make sure everything is running and connecting\n",
    "for my kubernetes spark setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9143c6a-1948-46ea-bdd3-085b7b472388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# load spark session templates\n",
    "from spark_utils import get_k8s_spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42a59fe-6a07-45ab-a48b-ec00f6253833",
   "metadata": {},
   "source": [
    "## Objectstore Tests \n",
    "\n",
    "we are using Minio as our object store so firstly lets test it independent of spark\n",
    "if we return buckets then all is good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a84ad64-524c-49a6-8f31-47dc9995c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "from minio.error import S3Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43f004-c109-41f3-9d17-d573fe7dea0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client = Minio(\n",
    "        \"minio.minio-tenant.svc.cluster.local\",\n",
    "        access_key='AKIAIOSFODNN7EXAMPLE',\n",
    "        secret_key='wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY',\n",
    "        secure=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25254032-5746-4c53-825d-5c7ad505d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = minio_client.list_buckets()\n",
    "\n",
    "for bucket in buckets:\n",
    "    print(bucket.name, bucket.creation_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe81ad-1996-4705-841d-e3495a79ead6",
   "metadata": {},
   "source": [
    "## Configs\n",
    "\n",
    "These configs are set to work with the stack at: https://github.com/Data-drone/data_eng_kube.git\n",
    "\n",
    "Note compared to Spark 2.x, Spark 3.x doesn't properly maven load spark.jars.packages:\n",
    "https://issues.apache.org/jira/browse/SPARK-35084\n",
    "\n",
    "We need to have at least the hadoop-aws jar already on drivers and executors to make things work more smoothly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8f680-0e54-4ac5-b000-0b8638a1fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_ARGS = \"--packages org.apache.hadoop:hadoop-aws:3.2.0,com.amazonaws:aws-java-sdk:1.12.79 --jars {0} \\\n",
    "--driver-class-path {1} pyspark-shell\".format(package_list, classPath)\n",
    "\n",
    "BASIC_SUBMIT_ARGS = \"--packages org.apache.hadoop:hadoop-aws:3.2.0 pyspark-shell\"\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = BASIC_SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed533883-3548-4047-9244-b427f9e12673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "access_key = 'AKIAIOSFODNN7EXAMPLE' # os.environ['MINIO_ACCESS_KEY']\n",
    "secret_key = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' # os.environ['MINIO_SECRET_KEY']\n",
    "\n",
    "spark = (get_k8s_spark()\n",
    "            .config(\"spark.kubernetes.container.image\", \n",
    "                    \"k3d-test-registry:5000/datadrone/k8s-spark-worker:3.1.2-hadoop3.2-rapids-k8s\")\n",
    "            .config(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", access_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio-tenant.svc.cluster.local\")\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            .config(\"spark.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\")\n",
    "            .appName(\"Spark K8s\")\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5287df9-0940-43c2-9411-a10d5869d5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check loaded jars\n",
    "print(spark.sparkContext._jsc.sc().listJars())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f7b761-4d7b-4e9b-aae1-41fe98be05fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test spark without reading data\n",
    "# Create a distributed data set to test to the session\n",
    "t = spark.sparkContext.parallelize(range(10))\n",
    "\n",
    "# Calculate the approximate sum of values in the dataset\n",
    "r = t.sumApprox(3)\n",
    "print('Approximate sum: %s' % r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca4a969-bfa3-4099-8290-3bb3df7532b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate some test data and run through Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256995b0-71c2-46a8-9d32-bc59a07ba33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0333e3-c9fe-44cb-9dc3-5002df62e27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.randn(100000,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d32d31-b7e2-4c57-8071-6ddb66c84b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fd3dfb-9a5e-431d-8299-8d4058ee840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF=spark.createDataFrame(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385c728-2580-40fb-a253-ca4e54c9b850",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a1a3e-518d-454e-ae54-ded860dfd3cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data and write it to my object store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e00de5-af43-48e2-9355-29076cbbaeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly create a new bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825d419-485b-40a6-a3b3-6fe4ebc40ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    minio_client.make_bucket('testing-bucket')\n",
    "except S3Error as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5afa5d-d0b8-4bc0-b590-03c8110f3427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need boto to pull from AWS\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5e4c8b-9db9-4094-b90d-6b09f3628233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore import UNSIGNED\n",
    "from botocore.client import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f891cb-7a57-4ace-b9f6-39dad1cc5e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a82020a-8917-4ab0-972c-7c0609aef79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "output_bucket = 'testing-bucket'\n",
    "testing_file = 'green_tripdata_2015-07.csv'\n",
    "load_path = 'trip data/' + testing_file\n",
    "write_path = 'raw_data/' + testing_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3965471-43ef-4ccd-8c0e-b2a86c040f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('green_tripdata_2015-07.csv', 'wb') as f:\n",
    "        s3.download_fileobj('nyc-tlc', load_path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743e8d4d-3100-4479-9888-c2e2fbe23828",
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_client.fput_object(output_bucket, write_path, testing_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3445c5e1-9abe-4e9f-8ce0-ab1ca2770b01",
   "metadata": {},
   "source": [
    "## Reading the loaded Data with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5879b-a8bb-4134-8735-d8447cd049f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spark.sparkContext.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152bb85e-7244-4edc-8a6b-e6b98cadc456",
   "metadata": {},
   "source": [
    "Test read from minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe7a62-0c48-4628-885e-6365fbc42944",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = spark.read.option(\"header\", True).csv(os.path.join('s3a://data/raw_data/green_tripdata_2014-09.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24060e42-2225-4f71-9e51-bddd7152e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4806803-d786-4318-a175-5ef8099f94b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44543f-ed98-44ed-a45a-8431c2c7c5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ceae63-6bcc-4352-bea2-353107922da5",
   "metadata": {},
   "source": [
    "# Close out Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300dfa4-24c9-41fa-a274-e52f8f546670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown Our Context\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f3c3f1-8f94-4962-9231-b7176d23f776",
   "metadata": {},
   "source": [
    "# Testing Different Class Paths and loading downloaded libs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b08daab-690c-42fa-9abc-4c251c7ef8f8",
   "metadata": {},
   "source": [
    "For larger libs we want to have them downloaded already and it would be good to be able to load libs from s3a paths so that we don't have to load workers and driver through local files.\n",
    "\n",
    "It seems like we need to add things in the `jars` section for pyspark to work properly. We also need it on local as the jars get loaded together. So it will get stuck if the hadoop-aws jars aren't loaded when it tries to load a s3 pathed one.\n",
    "Extra jars via `extraClassPath` don't seem to work either. Perhaps because it won't search through the classpaths on the initial spark initialisation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9067f3f7-eb90-4371-9ee3-35170197d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASIC_SUBMIT_ARGS = (\"--jars local:///opt/spark-jars/hadoop-aws-3.2.0.jar,\"\n",
    "                     \"local:///opt/spark-jars/delta-core_2.12-1.0.0.jar,\"\n",
    "                     \"local:///opt/spark-jars/aws-java-sdk-bundle-1.11.375.jar,\"\n",
    "                     \"local:///opt/sparkRapidsPlugin/cudf-21.08.2-cuda11.jar,\"\n",
    "                     \"local:///opt/sparkRapidsPlugin/rapids-4-spark_2.12-21.08.0.jar\"\n",
    "                     \" pyspark-shell\")\n",
    "\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = BASIC_SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deac0aad-9e45-4bb8-b29f-0cd4bc94f91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 12:22:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/03 12:23:20 WARN SQLExecPlugin: RAPIDS Accelerator 21.08.0 using cudf 21.08.2. To disable GPU support set `spark.rapids.sql.enabled` to false\n",
      "21/10/03 12:23:20 WARN Plugin: Installing rapids UDF compiler extensions to Spark. The compiler is disabled by default. To enable it, set `spark.rapids.sql.udfCompiler.enabled` to true\n"
     ]
    }
   ],
   "source": [
    "access_key = 'AKIAIOSFODNN7EXAMPLE' # os.environ['MINIO_ACCESS_KEY']\n",
    "secret_key = 'wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY' # os.environ['MINIO_SECRET_KEY']\n",
    "\n",
    "# This cannot be triggered in the python code as the JVM will be activated when it hits the python builder starts\n",
    "# .config(\"spark.packages\", \"org.apache.hadoop:hadoop-aws:3.2.0\")\n",
    "\n",
    "# debug\n",
    "# .config(\"spark.kubernetes.executor.deleteOnTermination\", \"false\")\n",
    "\n",
    "spark = (get_k8s_spark()\n",
    "            .config(\"spark.kubernetes.container.image\", \n",
    "                    \"k3d-test-registry:5000/datadrone/k8s-spark-worker:3.1.2-hadoop3.2-rapids-k8s\")\n",
    "            .config(\"spark.kubernetes.container.image.pullPolicy\", \"Always\")\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", access_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key)\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", \"minio.minio-tenant.svc.cluster.local\")\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "            .config(\"spark.executor.resource.gpu.amount\", \"1\")\n",
    "            .config(\"spark.task.resource.gpu.amount\", \"1\")\n",
    "            .config(\"spark.executor.resource.gpu.discoveryScript\", \"/opt/sparkRapidsPlugin/getGpusResources.sh\")\n",
    "            .config(\"spark.executor.resource.gpu.vendor\", \"nvidia.com\")\n",
    "            .config(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
    "            .config(\"spark.rapids.sql.concurrentGpuTasks\", \"2\")\n",
    "            .config(\"spark.kubernetes.executor.deleteOnTermination\", \"false\")\n",
    "            .appName(\"Spark K8s\")\n",
    "            .enableHiveSupport()\n",
    "            .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c8842a-2a67-4323-a909-a263525732d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 12:23:39 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "raw_data = spark.read.option(\"header\", True).csv(os.path.join('s3a://data/raw_data/green_tripdata_2014-09.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5721e83-ddae-4cde-a4d7-5a0f384756ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- Lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- Store_and_fwd_flag: string (nullable = true)\n",
      " |-- RateCodeID: string (nullable = true)\n",
      " |-- Pickup_longitude: string (nullable = true)\n",
      " |-- Pickup_latitude: string (nullable = true)\n",
      " |-- Dropoff_longitude: string (nullable = true)\n",
      " |-- Dropoff_latitude: string (nullable = true)\n",
      " |-- Passenger_count: string (nullable = true)\n",
      " |-- Trip_distance: string (nullable = true)\n",
      " |-- Fare_amount: string (nullable = true)\n",
      " |-- Extra: string (nullable = true)\n",
      " |-- MTA_tax: string (nullable = true)\n",
      " |-- Tip_amount: string (nullable = true)\n",
      " |-- Tolls_amount: string (nullable = true)\n",
      " |-- Ehail_fee: string (nullable = true)\n",
      " |-- Total_amount: string (nullable = true)\n",
      " |-- Payment_type: string (nullable = true)\n",
      " |-- Trip_type : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58ec7d45-7844-423c-a270-90bb1c4b60f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 12:23:51 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "21/10/03 12:25:49 WARN HeartbeatReceiver: Removing executor 1 with no recent heartbeats: 133229 ms exceeds timeout 120000 ms\n"
     ]
    }
   ],
   "source": [
    "clean_warehouse = \"s3a://data/warehouse/raw/green_taxi_pre2015\"\n",
    "\n",
    "delta_data = spark.read.option(\"header\", True).format(\"delta\").load(clean_warehouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfd4a5aa-1219-42df-918d-2ca629ad8204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- pickup_datetime: string (nullable = true)\n",
      " |-- dropoff_datetime: string (nullable = true)\n",
      " |-- Store_and_fwd_flag: string (nullable = true)\n",
      " |-- RateCodeID: string (nullable = true)\n",
      " |-- Pickup_longitude: string (nullable = true)\n",
      " |-- Pickup_latitude: string (nullable = true)\n",
      " |-- Dropoff_longitude: string (nullable = true)\n",
      " |-- Dropoff_latitude: string (nullable = true)\n",
      " |-- Passenger_count: string (nullable = true)\n",
      " |-- Trip_distance: string (nullable = true)\n",
      " |-- Fare_amount: string (nullable = true)\n",
      " |-- Extra: string (nullable = true)\n",
      " |-- MTA_tax: string (nullable = true)\n",
      " |-- Tip_amount: string (nullable = true)\n",
      " |-- Tolls_amount: string (nullable = true)\n",
      " |-- Ehail_fee: string (nullable = true)\n",
      " |-- Total_amount: string (nullable = true)\n",
      " |-- Payment_type: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc2a248f-a177-4208-a8c6-8bfb7cb7e336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/03 12:22:22 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\n"
     ]
    }
   ],
   "source": [
    "# Shutdown Our Context\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e37505-b93f-4e59-8682-4f5828827155",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark]",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
